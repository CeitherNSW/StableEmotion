{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T04:00:09.033718700Z",
     "start_time": "2023-07-24T04:00:08.512252600Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "from typing import LiteralString\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "folder_path = '../9517proj_sources/train'\n",
    "annotation_path = '../9517proj_sources/train_annotations'\n",
    "\n",
    "with open(annotation_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "for anno in annotations:\n",
    "    image_id = anno['image_id']\n",
    "    bbox = anno['bbox']\n",
    "    category_id = anno['category_id']\n",
    "    \n",
    "    # build image path, absolute path+ na me\n",
    "    image_file_path = os.path.join(folder_path, f\"image_id_{str(image_id).zfill(3)}.jpg\")\n",
    "    \n",
    "    image = cv2.imread(image_file_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"File not found: {image_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # draw bounding box\n",
    "    x, y, width, height = bbox\n",
    "    cv2.rectangle(image, (int(x), int(y)), (int(x+width), int(y+height)), (0, 255, 0), 2)\n",
    "    \n",
    "    # show image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T17:42:21.463797Z",
     "start_time": "2023-07-23T17:42:20.888342Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7693\n",
      "Epoch [1/10], Loss: 0.7586\n",
      "Epoch [1/10], Loss: 0.7417\n",
      "Epoch [1/10], Loss: 0.7265\n",
      "Epoch [1/10], Loss: 0.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:30,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7152\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_184.png'\n",
      "Epoch [2/10], Loss: 0.7031\n",
      "Epoch [2/10], Loss: 0.6843\n",
      "Epoch [2/10], Loss: 0.6777\n",
      "Epoch [2/10], Loss: 0.6676\n",
      "Epoch [2/10], Loss: 0.6558\n",
      "Epoch [2/10], Loss: 0.6406\n",
      "Epoch [2/10], Loss: 0.6398\n",
      "Epoch [2/10], Loss: 0.6300\n",
      "Epoch [2/10], Loss: 0.6320\n",
      "Epoch [2/10], Loss: 0.6132\n",
      "Epoch [2/10], Loss: 0.5859\n",
      "Epoch [2/10], Loss: 0.5893\n",
      "Epoch [2/10], Loss: 0.5789\n",
      "Epoch [2/10], Loss: 0.5713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:11<00:51,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.5687\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_465.png'\n",
      "Epoch [3/10], Loss: 0.5505\n",
      "Epoch [3/10], Loss: 0.5467\n",
      "Epoch [3/10], Loss: 0.5331\n",
      "Epoch [3/10], Loss: 0.5400\n",
      "Epoch [3/10], Loss: 0.5136\n",
      "Epoch [3/10], Loss: 0.5179\n",
      "Epoch [3/10], Loss: 0.5085\n",
      "Epoch [3/10], Loss: 0.4958\n",
      "Epoch [3/10], Loss: 0.4937\n",
      "Epoch [3/10], Loss: 0.4616\n",
      "Epoch [3/10], Loss: 0.4713\n",
      "Epoch [3/10], Loss: 0.4682\n",
      "Epoch [3/10], Loss: 0.4565\n",
      "Epoch [3/10], Loss: 0.4470\n",
      "Epoch [3/10], Loss: 0.4422\n",
      "Epoch [3/10], Loss: 0.4348\n",
      "Epoch [3/10], Loss: 0.4260\n",
      "Epoch [3/10], Loss: 0.4172\n",
      "Epoch [3/10], Loss: 0.4202\n",
      "Epoch [3/10], Loss: 0.4091\n",
      "Epoch [3/10], Loss: 0.3977\n",
      "Epoch [3/10], Loss: 0.3980\n",
      "Epoch [3/10], Loss: 0.3919\n",
      "Epoch [3/10], Loss: 0.3801\n",
      "Epoch [3/10], Loss: 0.3755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:26<01:10, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.3671\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_414.png'\n",
      "Epoch [4/10], Loss: 0.3663\n",
      "Epoch [4/10], Loss: 0.3575\n",
      "Epoch [4/10], Loss: 0.3514\n",
      "Epoch [4/10], Loss: 0.3541\n",
      "Epoch [4/10], Loss: 0.3433\n",
      "Epoch [4/10], Loss: 0.3428\n",
      "Epoch [4/10], Loss: 0.3257\n",
      "Epoch [4/10], Loss: 0.3264\n",
      "Epoch [4/10], Loss: 0.3200\n",
      "Epoch [4/10], Loss: 0.3122\n",
      "Epoch [4/10], Loss: 0.3119\n",
      "Epoch [4/10], Loss: 0.2996\n",
      "Epoch [4/10], Loss: 0.2961\n",
      "Epoch [4/10], Loss: 0.2990\n",
      "Epoch [4/10], Loss: 0.2906\n",
      "Epoch [4/10], Loss: 0.2964\n",
      "Epoch [4/10], Loss: 0.2885\n",
      "Epoch [4/10], Loss: 0.2715\n",
      "Epoch [4/10], Loss: 0.2639\n",
      "Epoch [4/10], Loss: 0.2669\n",
      "Epoch [4/10], Loss: 0.2635\n",
      "Epoch [4/10], Loss: 0.2612\n",
      "Epoch [4/10], Loss: 0.2485\n",
      "Epoch [4/10], Loss: 0.2499\n",
      "Epoch [4/10], Loss: 0.2468\n",
      "Epoch [4/10], Loss: 0.2386\n",
      "Epoch [4/10], Loss: 0.2400\n",
      "Epoch [4/10], Loss: 0.2338\n",
      "Epoch [4/10], Loss: 0.2247\n",
      "Epoch [4/10], Loss: 0.2314\n",
      "Epoch [4/10], Loss: 0.2246\n",
      "Epoch [4/10], Loss: 0.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:44<01:18, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.2165\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_478.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:44<00:43,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.2243\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_412.png'\n",
      "Epoch [6/10], Loss: 0.2090\n",
      "Epoch [6/10], Loss: 0.2143\n",
      "Epoch [6/10], Loss: 0.1980\n",
      "Epoch [6/10], Loss: 0.1934\n",
      "Epoch [6/10], Loss: 0.2059\n",
      "Epoch [6/10], Loss: 0.1998\n",
      "Epoch [6/10], Loss: 0.1939\n",
      "Epoch [6/10], Loss: 0.1891\n",
      "Epoch [6/10], Loss: 0.1856\n",
      "Epoch [6/10], Loss: 0.1782\n",
      "Epoch [6/10], Loss: 0.1730\n",
      "Epoch [6/10], Loss: 0.1903\n",
      "Epoch [6/10], Loss: 0.1847\n",
      "Epoch [6/10], Loss: 0.1783\n",
      "Epoch [6/10], Loss: 0.1641\n",
      "Epoch [6/10], Loss: 0.1691\n",
      "Epoch [6/10], Loss: 0.1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:54<00:35,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.1606\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_320.png'\n",
      "Epoch [7/10], Loss: 0.1516\n",
      "Epoch [7/10], Loss: 0.1594\n",
      "Epoch [7/10], Loss: 0.1530\n",
      "Epoch [7/10], Loss: 0.1534\n",
      "Epoch [7/10], Loss: 0.1543\n",
      "Epoch [7/10], Loss: 0.1523\n",
      "Epoch [7/10], Loss: 0.1512\n",
      "Epoch [7/10], Loss: 0.1467\n",
      "Epoch [7/10], Loss: 0.1399\n",
      "Epoch [7/10], Loss: 0.1459\n",
      "Epoch [7/10], Loss: 0.1440\n",
      "Epoch [7/10], Loss: 0.1424\n",
      "Epoch [7/10], Loss: 0.1346\n",
      "Epoch [7/10], Loss: 0.1324\n",
      "Epoch [7/10], Loss: 0.1377\n",
      "Epoch [7/10], Loss: 0.1305\n",
      "Epoch [7/10], Loss: 0.1297\n",
      "Epoch [7/10], Loss: 0.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:04<00:28,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.1189\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_412.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:05<00:13,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.1253\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_478.png'\n",
      "Epoch [9/10], Loss: 0.1327\n",
      "Epoch [9/10], Loss: 0.1190\n",
      "Epoch [9/10], Loss: 0.1248\n",
      "Epoch [9/10], Loss: 0.1211\n",
      "Epoch [9/10], Loss: 0.1129\n",
      "Epoch [9/10], Loss: 0.1153\n",
      "Epoch [9/10], Loss: 0.1103\n",
      "Epoch [9/10], Loss: 0.1121\n",
      "Epoch [9/10], Loss: 0.1123\n",
      "Epoch [9/10], Loss: 0.1072\n",
      "Epoch [9/10], Loss: 0.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:11<00:06,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.1028\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_414.png'\n",
      "Epoch [10/10], Loss: 0.1053\n",
      "Epoch [10/10], Loss: 0.1044\n",
      "Epoch [10/10], Loss: 0.1039\n",
      "Epoch [10/10], Loss: 0.1075\n",
      "Epoch [10/10], Loss: 0.1017\n",
      "Epoch [10/10], Loss: 0.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:15<00:00,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0996\n",
      "[Errno 2] No such file or directory: '../9517proj_sources/off\\\\image_id_478.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_folder, img_ext, mask_folder, mask_ext, transform=None, mask_transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.img_ext = img_ext\n",
    "        self.mask_folder = mask_folder\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.filenames = [os.path.splitext(filename)[0] for filename in os.listdir(img_folder)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_folder, self.filenames[idx] + self.img_ext)\n",
    "        mask_path = os.path.join(self.mask_folder, self.filenames[idx] + self.mask_ext)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # assuming masks are in 'L' mode\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = torch.squeeze(mask, 0)  # remove the first dimension (1, H, W) -> (H, W)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "path_folder = '../9517proj_sources/train'\n",
    "\n",
    "path_folder_2 = '../9517proj_sources/off'\n",
    "\n",
    "train_dataset = CustomDataset(path_folder, '.jpg', path_folder_2, '.png', transform=transform, mask_transform=mask_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# create model\n",
    "model = deeplabv3_resnet50(pretrained=False, progress=True, num_classes=2, aux_loss=None)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(10)):  # suppose we train for 10 epochs\n",
    "    try:\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks.long())  # note we need to convert masks to 'long' type\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print('Epoch [{}/10], Loss: {:.4f}'.format(epoch + 1, loss.item()))\n",
    "# for epoch in range(10):  # suppose we train for 10 epochs\n",
    "#     try:\n",
    "#         for images, masks in train_loader:\n",
    "#             print(images.shape, masks.shape)\n",
    "#             images = images.to('cuda') if torch.cuda.is_available() else images\n",
    "#             masks = masks.to('cuda') if torch.cuda.is_available() else masks\n",
    "#\n",
    "#             # forward pass\n",
    "#             outputs = model(images)['out']\n",
    "#             loss = criterion(outputs, masks.long())  # note we need to convert masks to 'long' type\n",
    "#\n",
    "#             # backward and optimize\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#\n",
    "#             print('Epoch [{}/10], Loss: {:.4f}'.format(epoch + 1, loss.item()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T03:22:18.287532Z",
     "start_time": "2023-07-24T03:21:02.165770800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7966\n",
      "Epoch [1/10], Loss: 0.7951\n",
      "Epoch [1/10], Loss: 0.7847\n",
      "Epoch [1/10], Loss: 0.7553\n",
      "Epoch [1/10], Loss: 0.7555\n",
      "Epoch [1/10], Loss: 0.7403\n",
      "Epoch [1/10], Loss: 0.7270\n",
      "Epoch [1/10], Loss: 0.7043\n",
      "Epoch [1/10], Loss: 0.7060\n",
      "Epoch [1/10], Loss: 0.6885\n",
      "Epoch [1/10], Loss: 0.6749\n",
      "Epoch [1/10], Loss: 0.6746\n",
      "Epoch [1/10], Loss: 0.6545\n",
      "Epoch [1/10], Loss: 0.6525\n",
      "Epoch [1/10], Loss: 0.6597\n",
      "Epoch [1/10], Loss: 0.6295\n",
      "Epoch [1/10], Loss: 0.6236\n",
      "Epoch [1/10], Loss: 0.6077\n",
      "Epoch [1/10], Loss: 0.5889\n",
      "Epoch [1/10], Loss: 0.5857\n",
      "Epoch [1/10], Loss: 0.5744\n",
      "Epoch [1/10], Loss: 0.5563\n",
      "Epoch [1/10], Loss: 0.5568\n",
      "Epoch [1/10], Loss: 0.5366\n",
      "Epoch [1/10], Loss: 0.5310\n",
      "Epoch [1/10], Loss: 0.5352\n",
      "Epoch [1/10], Loss: 0.5189\n",
      "Epoch [1/10], Loss: 0.5119\n",
      "Epoch [1/10], Loss: 0.4979\n",
      "Epoch [1/10], Loss: 0.4907\n",
      "Epoch [1/10], Loss: 0.4658\n",
      "Epoch [1/10], Loss: 0.4650\n",
      "Epoch [1/10], Loss: 0.4663\n",
      "Epoch [1/10], Loss: 0.4471\n",
      "Epoch [1/10], Loss: 0.4397\n",
      "Epoch [1/10], Loss: 0.4279\n",
      "Epoch [1/10], Loss: 0.4386\n",
      "Epoch [1/10], Loss: 0.4305\n",
      "Epoch [1/10], Loss: 0.4069\n",
      "Epoch [1/10], Loss: 0.3949\n",
      "Epoch [1/10], Loss: 0.4296\n",
      "Epoch [1/10], Loss: 0.3821\n",
      "Epoch [1/10], Loss: 0.4015\n",
      "Epoch [1/10], Loss: 0.3831\n",
      "Epoch [1/10], Loss: 0.3685\n",
      "Epoch [1/10], Loss: 0.3621\n",
      "Epoch [1/10], Loss: 0.3507\n",
      "Epoch [1/10], Loss: 0.3511\n",
      "Epoch [1/10], Loss: 0.3354\n",
      "Epoch [1/10], Loss: 0.3235\n",
      "Epoch [1/10], Loss: 0.3311\n",
      "Epoch [1/10], Loss: 0.3224\n",
      "Epoch [1/10], Loss: 0.3113\n",
      "Epoch [1/10], Loss: 0.3047\n",
      "Epoch [1/10], Loss: 0.2941\n",
      "Epoch [1/10], Loss: 0.3008\n",
      "Epoch [1/10], Loss: 0.2897\n",
      "Epoch [1/10], Loss: 0.2883\n",
      "Epoch [1/10], Loss: 0.2996\n",
      "Epoch [1/10], Loss: 0.2673\n",
      "Epoch [1/10], Loss: 0.2512\n",
      "Epoch [1/10], Loss: 0.2519\n",
      "Epoch [1/10], Loss: 0.2513\n",
      "Epoch [1/10], Loss: 0.2538\n",
      "Epoch [1/10], Loss: 0.2425\n",
      "Epoch [1/10], Loss: 0.2448\n",
      "Epoch [1/10], Loss: 0.2457\n",
      "Epoch [1/10], Loss: 0.2358\n",
      "Epoch [1/10], Loss: 0.2466\n",
      "Epoch [1/10], Loss: 0.2326\n",
      "Epoch [1/10], Loss: 0.2200\n",
      "Epoch [1/10], Loss: 0.2377\n",
      "Epoch [1/10], Loss: 0.2161\n",
      "Epoch [1/10], Loss: 0.2336\n",
      "Epoch [1/10], Loss: 0.2245\n",
      "Epoch [1/10], Loss: 0.2208\n",
      "Epoch [1/10], Loss: 0.1974\n",
      "Epoch [1/10], Loss: 0.2008\n",
      "Epoch [1/10], Loss: 0.2030\n",
      "Epoch [1/10], Loss: 0.1856\n",
      "Epoch [1/10], Loss: 0.1832\n",
      "Epoch [1/10], Loss: 0.2009\n",
      "Epoch [1/10], Loss: 0.1753\n",
      "Epoch [1/10], Loss: 0.1862\n",
      "Epoch [1/10], Loss: 0.1766\n",
      "Epoch [1/10], Loss: 0.1700\n",
      "Epoch [1/10], Loss: 0.1681\n",
      "Epoch [1/10], Loss: 0.1739\n",
      "Epoch [1/10], Loss: 0.1734\n",
      "Epoch [1/10], Loss: 0.1718\n",
      "Epoch [1/10], Loss: 0.1716\n",
      "Epoch [1/10], Loss: 0.1618\n",
      "Epoch [1/10], Loss: 0.1548\n",
      "Epoch [1/10], Loss: 0.1587\n",
      "Epoch [1/10], Loss: 0.1529\n",
      "Epoch [1/10], Loss: 0.1492\n",
      "Epoch [1/10], Loss: 0.1615\n",
      "Epoch [1/10], Loss: 0.1612\n",
      "Epoch [1/10], Loss: 0.1527\n",
      "Epoch [1/10], Loss: 0.1582\n",
      "Epoch [1/10], Loss: 0.1377\n",
      "Epoch [1/10], Loss: 0.1634\n",
      "Epoch [1/10], Loss: 0.1674\n",
      "Epoch [1/10], Loss: 0.1397\n",
      "Epoch [1/10], Loss: 0.1385\n",
      "Epoch [1/10], Loss: 0.1280\n",
      "Epoch [1/10], Loss: 0.1333\n",
      "Epoch [1/10], Loss: 0.1304\n",
      "Epoch [1/10], Loss: 0.1371\n",
      "Epoch [1/10], Loss: 0.1306\n",
      "Epoch [1/10], Loss: 0.1293\n",
      "Epoch [1/10], Loss: 0.1185\n",
      "Epoch [1/10], Loss: 0.1342\n",
      "Epoch [1/10], Loss: 0.1222\n",
      "Epoch [1/10], Loss: 0.1215\n",
      "Epoch [1/10], Loss: 0.1250\n",
      "Epoch [1/10], Loss: 0.1461\n",
      "Epoch [1/10], Loss: 0.1106\n",
      "Epoch [1/10], Loss: 0.1213\n",
      "Epoch [1/10], Loss: 0.1182\n",
      "Epoch [1/10], Loss: 0.1231\n",
      "Epoch [2/10], Loss: 0.1166\n",
      "Epoch [2/10], Loss: 0.1161\n",
      "Epoch [2/10], Loss: 0.1154\n",
      "Epoch [2/10], Loss: 0.1089\n",
      "Epoch [2/10], Loss: 0.1086\n",
      "Epoch [2/10], Loss: 0.1058\n",
      "Epoch [2/10], Loss: 0.1155\n",
      "Epoch [2/10], Loss: 0.1122\n",
      "Epoch [2/10], Loss: 0.1047\n",
      "Epoch [2/10], Loss: 0.1052\n",
      "Epoch [2/10], Loss: 0.1047\n",
      "Epoch [2/10], Loss: 0.1040\n",
      "Epoch [2/10], Loss: 0.1034\n",
      "Epoch [2/10], Loss: 0.1017\n",
      "Epoch [2/10], Loss: 0.1167\n",
      "Epoch [2/10], Loss: 0.1076\n",
      "Epoch [2/10], Loss: 0.0898\n",
      "Epoch [2/10], Loss: 0.0933\n",
      "Epoch [2/10], Loss: 0.0948\n",
      "Epoch [2/10], Loss: 0.0991\n",
      "Epoch [2/10], Loss: 0.1080\n",
      "Epoch [2/10], Loss: 0.0937\n",
      "Epoch [2/10], Loss: 0.0908\n",
      "Epoch [2/10], Loss: 0.0916\n",
      "Epoch [2/10], Loss: 0.1119\n",
      "Epoch [2/10], Loss: 0.0865\n",
      "Epoch [2/10], Loss: 0.0867\n",
      "Epoch [2/10], Loss: 0.0827\n",
      "Epoch [2/10], Loss: 0.0934\n",
      "Epoch [2/10], Loss: 0.0858\n",
      "Epoch [2/10], Loss: 0.0838\n",
      "Epoch [2/10], Loss: 0.1034\n",
      "Epoch [2/10], Loss: 0.0821\n",
      "Epoch [2/10], Loss: 0.0845\n",
      "Epoch [2/10], Loss: 0.0826\n",
      "Epoch [2/10], Loss: 0.0967\n",
      "Epoch [2/10], Loss: 0.0797\n",
      "Epoch [2/10], Loss: 0.0833\n",
      "Epoch [2/10], Loss: 0.0870\n",
      "Epoch [2/10], Loss: 0.0770\n",
      "Epoch [2/10], Loss: 0.0808\n",
      "Epoch [2/10], Loss: 0.0863\n",
      "Epoch [2/10], Loss: 0.0746\n",
      "Epoch [2/10], Loss: 0.0758\n",
      "Epoch [2/10], Loss: 0.0800\n",
      "Epoch [2/10], Loss: 0.0830\n",
      "Epoch [2/10], Loss: 0.0772\n",
      "Epoch [2/10], Loss: 0.0813\n",
      "Epoch [2/10], Loss: 0.0769\n",
      "Epoch [2/10], Loss: 0.0758\n",
      "Epoch [2/10], Loss: 0.0758\n",
      "Epoch [2/10], Loss: 0.0708\n",
      "Epoch [2/10], Loss: 0.0686\n",
      "Epoch [2/10], Loss: 0.0824\n",
      "Epoch [2/10], Loss: 0.0725\n",
      "Epoch [2/10], Loss: 0.0714\n",
      "Epoch [2/10], Loss: 0.0741\n",
      "Epoch [2/10], Loss: 0.0725\n",
      "Epoch [2/10], Loss: 0.0731\n",
      "Epoch [2/10], Loss: 0.0638\n",
      "Epoch [2/10], Loss: 0.0652\n",
      "Epoch [2/10], Loss: 0.0661\n",
      "Epoch [2/10], Loss: 0.0683\n",
      "Epoch [2/10], Loss: 0.0661\n",
      "Epoch [2/10], Loss: 0.0735\n",
      "Epoch [2/10], Loss: 0.0703\n",
      "Epoch [2/10], Loss: 0.0782\n",
      "Epoch [2/10], Loss: 0.0643\n",
      "Epoch [2/10], Loss: 0.0631\n",
      "Epoch [2/10], Loss: 0.0661\n",
      "Epoch [2/10], Loss: 0.0628\n",
      "Epoch [2/10], Loss: 0.0661\n",
      "Epoch [2/10], Loss: 0.0625\n",
      "Epoch [2/10], Loss: 0.0604\n",
      "Epoch [2/10], Loss: 0.0613\n",
      "Epoch [2/10], Loss: 0.0585\n",
      "Epoch [2/10], Loss: 0.0613\n",
      "Epoch [2/10], Loss: 0.0745\n",
      "Epoch [2/10], Loss: 0.0584\n",
      "Epoch [2/10], Loss: 0.0579\n",
      "Epoch [2/10], Loss: 0.0564\n",
      "Epoch [2/10], Loss: 0.0544\n",
      "Epoch [2/10], Loss: 0.0580\n",
      "Epoch [2/10], Loss: 0.0555\n",
      "Epoch [2/10], Loss: 0.0553\n",
      "Epoch [2/10], Loss: 0.0558\n",
      "Epoch [2/10], Loss: 0.0733\n",
      "Epoch [2/10], Loss: 0.0529\n",
      "Epoch [2/10], Loss: 0.0526\n",
      "Epoch [2/10], Loss: 0.0535\n",
      "Epoch [2/10], Loss: 0.0550\n",
      "Epoch [2/10], Loss: 0.0540\n",
      "Epoch [2/10], Loss: 0.0576\n",
      "Epoch [2/10], Loss: 0.0486\n",
      "Epoch [2/10], Loss: 0.0530\n",
      "Epoch [2/10], Loss: 0.0499\n",
      "Epoch [2/10], Loss: 0.0519\n",
      "Epoch [2/10], Loss: 0.0538\n",
      "Epoch [2/10], Loss: 0.0520\n",
      "Epoch [2/10], Loss: 0.0532\n",
      "Epoch [2/10], Loss: 0.0538\n",
      "Epoch [2/10], Loss: 0.0494\n",
      "Epoch [2/10], Loss: 0.0497\n",
      "Epoch [2/10], Loss: 0.0601\n",
      "Epoch [2/10], Loss: 0.0506\n",
      "Epoch [2/10], Loss: 0.0466\n",
      "Epoch [2/10], Loss: 0.0446\n",
      "Epoch [2/10], Loss: 0.0467\n",
      "Epoch [2/10], Loss: 0.0461\n",
      "Epoch [2/10], Loss: 0.0474\n",
      "Epoch [2/10], Loss: 0.0488\n",
      "Epoch [2/10], Loss: 0.0457\n",
      "Epoch [2/10], Loss: 0.0460\n",
      "Epoch [2/10], Loss: 0.0446\n",
      "Epoch [2/10], Loss: 0.0490\n",
      "Epoch [2/10], Loss: 0.0463\n",
      "Epoch [2/10], Loss: 0.0462\n",
      "Epoch [2/10], Loss: 0.0462\n",
      "Epoch [2/10], Loss: 0.0413\n",
      "Epoch [2/10], Loss: 0.0443\n",
      "Epoch [2/10], Loss: 0.0610\n",
      "Epoch [3/10], Loss: 0.0485\n",
      "Epoch [3/10], Loss: 0.0460\n",
      "Epoch [3/10], Loss: 0.0449\n",
      "Epoch [3/10], Loss: 0.0414\n",
      "Epoch [3/10], Loss: 0.0537\n",
      "Epoch [3/10], Loss: 0.0426\n",
      "Epoch [3/10], Loss: 0.0427\n",
      "Epoch [3/10], Loss: 0.0412\n",
      "Epoch [3/10], Loss: 0.0404\n",
      "Epoch [3/10], Loss: 0.0409\n",
      "Epoch [3/10], Loss: 0.0418\n",
      "Epoch [3/10], Loss: 0.0393\n",
      "Epoch [3/10], Loss: 0.0428\n",
      "Epoch [3/10], Loss: 0.0419\n",
      "Epoch [3/10], Loss: 0.0421\n",
      "Epoch [3/10], Loss: 0.0411\n",
      "Epoch [3/10], Loss: 0.0398\n",
      "Epoch [3/10], Loss: 0.0389\n",
      "Epoch [3/10], Loss: 0.0389\n",
      "Epoch [3/10], Loss: 0.0397\n",
      "Epoch [3/10], Loss: 0.0382\n",
      "Epoch [3/10], Loss: 0.0365\n",
      "Epoch [3/10], Loss: 0.0375\n",
      "Epoch [3/10], Loss: 0.0376\n",
      "Epoch [3/10], Loss: 0.0363\n",
      "Epoch [3/10], Loss: 0.0364\n",
      "Epoch [3/10], Loss: 0.0383\n",
      "Epoch [3/10], Loss: 0.0363\n",
      "Epoch [3/10], Loss: 0.0374\n",
      "Epoch [3/10], Loss: 0.0351\n",
      "Epoch [3/10], Loss: 0.0356\n",
      "Epoch [3/10], Loss: 0.0351\n",
      "Epoch [3/10], Loss: 0.0368\n",
      "Epoch [3/10], Loss: 0.0354\n",
      "Epoch [3/10], Loss: 0.0341\n",
      "Epoch [3/10], Loss: 0.0353\n",
      "Epoch [3/10], Loss: 0.0357\n",
      "Epoch [3/10], Loss: 0.0347\n",
      "Epoch [3/10], Loss: 0.0521\n",
      "Epoch [3/10], Loss: 0.0347\n",
      "Epoch [3/10], Loss: 0.0356\n",
      "Epoch [3/10], Loss: 0.0528\n",
      "Epoch [3/10], Loss: 0.0324\n",
      "Epoch [3/10], Loss: 0.0328\n",
      "Epoch [3/10], Loss: 0.0343\n",
      "Epoch [3/10], Loss: 0.0342\n",
      "Epoch [3/10], Loss: 0.0327\n",
      "Epoch [3/10], Loss: 0.0330\n",
      "Epoch [3/10], Loss: 0.0332\n",
      "Epoch [3/10], Loss: 0.0336\n",
      "Epoch [3/10], Loss: 0.0336\n",
      "Epoch [3/10], Loss: 0.0320\n",
      "Epoch [3/10], Loss: 0.0326\n",
      "Epoch [3/10], Loss: 0.0322\n",
      "Epoch [3/10], Loss: 0.0436\n",
      "Epoch [3/10], Loss: 0.0299\n",
      "Epoch [3/10], Loss: 0.0314\n",
      "Epoch [3/10], Loss: 0.0449\n",
      "Epoch [3/10], Loss: 0.0309\n",
      "Epoch [3/10], Loss: 0.0321\n",
      "Epoch [3/10], Loss: 0.0313\n",
      "Epoch [3/10], Loss: 0.0317\n",
      "Epoch [3/10], Loss: 0.0422\n",
      "Epoch [3/10], Loss: 0.0309\n",
      "Epoch [3/10], Loss: 0.0290\n",
      "Epoch [3/10], Loss: 0.0304\n",
      "Epoch [3/10], Loss: 0.0294\n",
      "Epoch [3/10], Loss: 0.0434\n",
      "Epoch [3/10], Loss: 0.0295\n",
      "Epoch [3/10], Loss: 0.0284\n",
      "Epoch [3/10], Loss: 0.0293\n",
      "Epoch [3/10], Loss: 0.0292\n",
      "Epoch [3/10], Loss: 0.0288\n",
      "Epoch [3/10], Loss: 0.0388\n",
      "Epoch [3/10], Loss: 0.0320\n",
      "Epoch [3/10], Loss: 0.0292\n",
      "Epoch [3/10], Loss: 0.0295\n",
      "Epoch [3/10], Loss: 0.0286\n",
      "Epoch [3/10], Loss: 0.0274\n",
      "Epoch [3/10], Loss: 0.0272\n",
      "Epoch [3/10], Loss: 0.0281\n",
      "Epoch [3/10], Loss: 0.0286\n",
      "Epoch [3/10], Loss: 0.0277\n",
      "Epoch [3/10], Loss: 0.0268\n",
      "Epoch [3/10], Loss: 0.0274\n",
      "Epoch [3/10], Loss: 0.0270\n",
      "Epoch [3/10], Loss: 0.0271\n",
      "Epoch [3/10], Loss: 0.0272\n",
      "Epoch [3/10], Loss: 0.0268\n",
      "Epoch [3/10], Loss: 0.0284\n",
      "Epoch [3/10], Loss: 0.0255\n",
      "Epoch [3/10], Loss: 0.0251\n",
      "Epoch [3/10], Loss: 0.0255\n",
      "Epoch [3/10], Loss: 0.0263\n",
      "Epoch [3/10], Loss: 0.0327\n",
      "Epoch [3/10], Loss: 0.0263\n",
      "Epoch [3/10], Loss: 0.0259\n",
      "Epoch [3/10], Loss: 0.0246\n",
      "Epoch [3/10], Loss: 0.0260\n",
      "Epoch [3/10], Loss: 0.0260\n",
      "Epoch [3/10], Loss: 0.0245\n",
      "Epoch [3/10], Loss: 0.0252\n",
      "Epoch [3/10], Loss: 0.0264\n",
      "Epoch [3/10], Loss: 0.0255\n",
      "Epoch [3/10], Loss: 0.0252\n",
      "Epoch [3/10], Loss: 0.0246\n",
      "Epoch [3/10], Loss: 0.0251\n",
      "Epoch [3/10], Loss: 0.0241\n",
      "Epoch [3/10], Loss: 0.0236\n",
      "Epoch [3/10], Loss: 0.0255\n",
      "Epoch [3/10], Loss: 0.0252\n",
      "Epoch [3/10], Loss: 0.0240\n",
      "Epoch [3/10], Loss: 0.0239\n",
      "Epoch [3/10], Loss: 0.0243\n",
      "Epoch [3/10], Loss: 0.0243\n",
      "Epoch [3/10], Loss: 0.0231\n",
      "Epoch [3/10], Loss: 0.0232\n",
      "Epoch [3/10], Loss: 0.0226\n",
      "Epoch [3/10], Loss: 0.0241\n",
      "Epoch [3/10], Loss: 0.0245\n",
      "Epoch [3/10], Loss: 0.0335\n",
      "Epoch [4/10], Loss: 0.0233\n",
      "Epoch [4/10], Loss: 0.0229\n",
      "Epoch [4/10], Loss: 0.0235\n",
      "Epoch [4/10], Loss: 0.0349\n",
      "Epoch [4/10], Loss: 0.0238\n",
      "Epoch [4/10], Loss: 0.0223\n",
      "Epoch [4/10], Loss: 0.0227\n",
      "Epoch [4/10], Loss: 0.0226\n",
      "Epoch [4/10], Loss: 0.0284\n",
      "Epoch [4/10], Loss: 0.0273\n",
      "Epoch [4/10], Loss: 0.0231\n",
      "Epoch [4/10], Loss: 0.0217\n",
      "Epoch [4/10], Loss: 0.0224\n",
      "Epoch [4/10], Loss: 0.0221\n",
      "Epoch [4/10], Loss: 0.0207\n",
      "Epoch [4/10], Loss: 0.0216\n",
      "Epoch [4/10], Loss: 0.0250\n",
      "Epoch [4/10], Loss: 0.0213\n",
      "Epoch [4/10], Loss: 0.0211\n",
      "Epoch [4/10], Loss: 0.0205\n",
      "Epoch [4/10], Loss: 0.0225\n",
      "Epoch [4/10], Loss: 0.0223\n",
      "Epoch [4/10], Loss: 0.0211\n",
      "Epoch [4/10], Loss: 0.0219\n",
      "Epoch [4/10], Loss: 0.0207\n",
      "Epoch [4/10], Loss: 0.0200\n",
      "Epoch [4/10], Loss: 0.0201\n",
      "Epoch [4/10], Loss: 0.0205\n",
      "Epoch [4/10], Loss: 0.0202\n",
      "Epoch [4/10], Loss: 0.0209\n",
      "Epoch [4/10], Loss: 0.0196\n",
      "Epoch [4/10], Loss: 0.0203\n",
      "Epoch [4/10], Loss: 0.0208\n",
      "Epoch [4/10], Loss: 0.0195\n",
      "Epoch [4/10], Loss: 0.0207\n",
      "Epoch [4/10], Loss: 0.0226\n",
      "Epoch [4/10], Loss: 0.0196\n",
      "Epoch [4/10], Loss: 0.0200\n",
      "Epoch [4/10], Loss: 0.0197\n",
      "Epoch [4/10], Loss: 0.0220\n",
      "Epoch [4/10], Loss: 0.0198\n",
      "Epoch [4/10], Loss: 0.0196\n",
      "Epoch [4/10], Loss: 0.0204\n",
      "Epoch [4/10], Loss: 0.0182\n",
      "Epoch [4/10], Loss: 0.0192\n",
      "Epoch [4/10], Loss: 0.0190\n",
      "Epoch [4/10], Loss: 0.0190\n",
      "Epoch [4/10], Loss: 0.0184\n",
      "Epoch [4/10], Loss: 0.0182\n",
      "Epoch [4/10], Loss: 0.0182\n",
      "Epoch [4/10], Loss: 0.0181\n",
      "Epoch [4/10], Loss: 0.0185\n",
      "Epoch [4/10], Loss: 0.0194\n",
      "Epoch [4/10], Loss: 0.0191\n",
      "Epoch [4/10], Loss: 0.0185\n",
      "Epoch [4/10], Loss: 0.0178\n",
      "Epoch [4/10], Loss: 0.0176\n",
      "Epoch [4/10], Loss: 0.0184\n",
      "Epoch [4/10], Loss: 0.0183\n",
      "Epoch [4/10], Loss: 0.0187\n",
      "Epoch [4/10], Loss: 0.0177\n",
      "Epoch [4/10], Loss: 0.0185\n",
      "Epoch [4/10], Loss: 0.0176\n",
      "Epoch [4/10], Loss: 0.0178\n",
      "Epoch [4/10], Loss: 0.0177\n",
      "Epoch [4/10], Loss: 0.0178\n",
      "Epoch [4/10], Loss: 0.0422\n",
      "Epoch [4/10], Loss: 0.0183\n",
      "Epoch [4/10], Loss: 0.0171\n",
      "Epoch [4/10], Loss: 0.0176\n",
      "Epoch [4/10], Loss: 0.0180\n",
      "Epoch [4/10], Loss: 0.0196\n",
      "Epoch [4/10], Loss: 0.0171\n",
      "Epoch [4/10], Loss: 0.0176\n",
      "Epoch [4/10], Loss: 0.0275\n",
      "Epoch [4/10], Loss: 0.0168\n",
      "Epoch [4/10], Loss: 0.0171\n",
      "Epoch [4/10], Loss: 0.0172\n",
      "Epoch [4/10], Loss: 0.0173\n",
      "Epoch [4/10], Loss: 0.0179\n",
      "Epoch [4/10], Loss: 0.0177\n",
      "Epoch [4/10], Loss: 0.0172\n",
      "Epoch [4/10], Loss: 0.0175\n",
      "Epoch [4/10], Loss: 0.0157\n",
      "Epoch [4/10], Loss: 0.0166\n",
      "Epoch [4/10], Loss: 0.0220\n",
      "Epoch [4/10], Loss: 0.0172\n",
      "Epoch [4/10], Loss: 0.0172\n",
      "Epoch [4/10], Loss: 0.0167\n",
      "Epoch [4/10], Loss: 0.0156\n",
      "Epoch [4/10], Loss: 0.0157\n",
      "Epoch [4/10], Loss: 0.0168\n",
      "Epoch [4/10], Loss: 0.0161\n",
      "Epoch [4/10], Loss: 0.0170\n",
      "Epoch [4/10], Loss: 0.0159\n",
      "Epoch [4/10], Loss: 0.0168\n",
      "Epoch [4/10], Loss: 0.0161\n",
      "Epoch [4/10], Loss: 0.0151\n",
      "Epoch [4/10], Loss: 0.0170\n",
      "Epoch [4/10], Loss: 0.0162\n",
      "Epoch [4/10], Loss: 0.0160\n",
      "Epoch [4/10], Loss: 0.0152\n",
      "Epoch [4/10], Loss: 0.0162\n",
      "Epoch [4/10], Loss: 0.0169\n",
      "Epoch [4/10], Loss: 0.0176\n",
      "Epoch [4/10], Loss: 0.0151\n",
      "Epoch [4/10], Loss: 0.0177\n",
      "Epoch [4/10], Loss: 0.0153\n",
      "Epoch [4/10], Loss: 0.0151\n",
      "Epoch [4/10], Loss: 0.0149\n",
      "Epoch [4/10], Loss: 0.0150\n",
      "Epoch [4/10], Loss: 0.0155\n",
      "Epoch [4/10], Loss: 0.0155\n",
      "Epoch [4/10], Loss: 0.0169\n",
      "Epoch [4/10], Loss: 0.0162\n",
      "Epoch [4/10], Loss: 0.0142\n",
      "Epoch [4/10], Loss: 0.0149\n",
      "Epoch [4/10], Loss: 0.0147\n",
      "Epoch [4/10], Loss: 0.0149\n",
      "Epoch [4/10], Loss: 0.0152\n",
      "Epoch [4/10], Loss: 0.0267\n",
      "Epoch [5/10], Loss: 0.0238\n",
      "Epoch [5/10], Loss: 0.0148\n",
      "Epoch [5/10], Loss: 0.0146\n",
      "Epoch [5/10], Loss: 0.0150\n",
      "Epoch [5/10], Loss: 0.0147\n",
      "Epoch [5/10], Loss: 0.0143\n",
      "Epoch [5/10], Loss: 0.0139\n",
      "Epoch [5/10], Loss: 0.0147\n",
      "Epoch [5/10], Loss: 0.0140\n",
      "Epoch [5/10], Loss: 0.0146\n",
      "Epoch [5/10], Loss: 0.0143\n",
      "Epoch [5/10], Loss: 0.0169\n",
      "Epoch [5/10], Loss: 0.0148\n",
      "Epoch [5/10], Loss: 0.0137\n",
      "Epoch [5/10], Loss: 0.0144\n",
      "Epoch [5/10], Loss: 0.0149\n",
      "Epoch [5/10], Loss: 0.0144\n",
      "Epoch [5/10], Loss: 0.0143\n",
      "Epoch [5/10], Loss: 0.0137\n",
      "Epoch [5/10], Loss: 0.0139\n",
      "Epoch [5/10], Loss: 0.0211\n",
      "Epoch [5/10], Loss: 0.0148\n",
      "Epoch [5/10], Loss: 0.0135\n",
      "Epoch [5/10], Loss: 0.0135\n",
      "Epoch [5/10], Loss: 0.0133\n",
      "Epoch [5/10], Loss: 0.0134\n",
      "Epoch [5/10], Loss: 0.0130\n",
      "Epoch [5/10], Loss: 0.0135\n",
      "Epoch [5/10], Loss: 0.0140\n",
      "Epoch [5/10], Loss: 0.0136\n",
      "Epoch [5/10], Loss: 0.0143\n",
      "Epoch [5/10], Loss: 0.0137\n",
      "Epoch [5/10], Loss: 0.0128\n",
      "Epoch [5/10], Loss: 0.0127\n",
      "Epoch [5/10], Loss: 0.0138\n",
      "Epoch [5/10], Loss: 0.0131\n",
      "Epoch [5/10], Loss: 0.0131\n",
      "Epoch [5/10], Loss: 0.0126\n",
      "Epoch [5/10], Loss: 0.0132\n",
      "Epoch [5/10], Loss: 0.0128\n",
      "Epoch [5/10], Loss: 0.0135\n",
      "Epoch [5/10], Loss: 0.0149\n",
      "Epoch [5/10], Loss: 0.0130\n",
      "Epoch [5/10], Loss: 0.0138\n",
      "Epoch [5/10], Loss: 0.0126\n",
      "Epoch [5/10], Loss: 0.0128\n",
      "Epoch [5/10], Loss: 0.0129\n",
      "Epoch [5/10], Loss: 0.0129\n",
      "Epoch [5/10], Loss: 0.0125\n",
      "Epoch [5/10], Loss: 0.0123\n",
      "Epoch [5/10], Loss: 0.0129\n",
      "Epoch [5/10], Loss: 0.0121\n",
      "Epoch [5/10], Loss: 0.0123\n",
      "Epoch [5/10], Loss: 0.0125\n",
      "Epoch [5/10], Loss: 0.0126\n",
      "Epoch [5/10], Loss: 0.0210\n",
      "Epoch [5/10], Loss: 0.0125\n",
      "Epoch [5/10], Loss: 0.0118\n",
      "Epoch [5/10], Loss: 0.0126\n",
      "Epoch [5/10], Loss: 0.0206\n",
      "Epoch [5/10], Loss: 0.0117\n",
      "Epoch [5/10], Loss: 0.0121\n",
      "Epoch [5/10], Loss: 0.0122\n",
      "Epoch [5/10], Loss: 0.0123\n",
      "Epoch [5/10], Loss: 0.0120\n",
      "Epoch [5/10], Loss: 0.0119\n",
      "Epoch [5/10], Loss: 0.0124\n",
      "Epoch [5/10], Loss: 0.0116\n",
      "Epoch [5/10], Loss: 0.0123\n",
      "Epoch [5/10], Loss: 0.0137\n",
      "Epoch [5/10], Loss: 0.0133\n",
      "Epoch [5/10], Loss: 0.0117\n",
      "Epoch [5/10], Loss: 0.0118\n",
      "Epoch [5/10], Loss: 0.0122\n",
      "Epoch [5/10], Loss: 0.0113\n",
      "Epoch [5/10], Loss: 0.0117\n",
      "Epoch [5/10], Loss: 0.0116\n",
      "Epoch [5/10], Loss: 0.0114\n",
      "Epoch [5/10], Loss: 0.0119\n",
      "Epoch [5/10], Loss: 0.0119\n",
      "Epoch [5/10], Loss: 0.0110\n",
      "Epoch [5/10], Loss: 0.0120\n",
      "Epoch [5/10], Loss: 0.0116\n",
      "Epoch [5/10], Loss: 0.0115\n",
      "Epoch [5/10], Loss: 0.0116\n",
      "Epoch [5/10], Loss: 0.0136\n",
      "Epoch [5/10], Loss: 0.0116\n",
      "Epoch [5/10], Loss: 0.0112\n",
      "Epoch [5/10], Loss: 0.0109\n",
      "Epoch [5/10], Loss: 0.0112\n",
      "Epoch [5/10], Loss: 0.0113\n",
      "Epoch [5/10], Loss: 0.0117\n",
      "Epoch [5/10], Loss: 0.0106\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0113\n",
      "Epoch [5/10], Loss: 0.0117\n",
      "Epoch [5/10], Loss: 0.0109\n",
      "Epoch [5/10], Loss: 0.0110\n",
      "Epoch [5/10], Loss: 0.0106\n",
      "Epoch [5/10], Loss: 0.0106\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0102\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0111\n",
      "Epoch [5/10], Loss: 0.0110\n",
      "Epoch [5/10], Loss: 0.0103\n",
      "Epoch [5/10], Loss: 0.0109\n",
      "Epoch [5/10], Loss: 0.0103\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0104\n",
      "Epoch [5/10], Loss: 0.0105\n",
      "Epoch [5/10], Loss: 0.0105\n",
      "Epoch [5/10], Loss: 0.0111\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0158\n",
      "Epoch [5/10], Loss: 0.0108\n",
      "Epoch [5/10], Loss: 0.0106\n",
      "Epoch [5/10], Loss: 0.0107\n",
      "Epoch [5/10], Loss: 0.0115\n",
      "Epoch [5/10], Loss: 0.0185\n",
      "Epoch [6/10], Loss: 0.0103\n",
      "Epoch [6/10], Loss: 0.0106\n",
      "Epoch [6/10], Loss: 0.0138\n",
      "Epoch [6/10], Loss: 0.0100\n",
      "Epoch [6/10], Loss: 0.0104\n",
      "Epoch [6/10], Loss: 0.0103\n",
      "Epoch [6/10], Loss: 0.0099\n",
      "Epoch [6/10], Loss: 0.0107\n",
      "Epoch [6/10], Loss: 0.0108\n",
      "Epoch [6/10], Loss: 0.0110\n",
      "Epoch [6/10], Loss: 0.0113\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0101\n",
      "Epoch [6/10], Loss: 0.0113\n",
      "Epoch [6/10], Loss: 0.0120\n",
      "Epoch [6/10], Loss: 0.0101\n",
      "Epoch [6/10], Loss: 0.0099\n",
      "Epoch [6/10], Loss: 0.0103\n",
      "Epoch [6/10], Loss: 0.0105\n",
      "Epoch [6/10], Loss: 0.0105\n",
      "Epoch [6/10], Loss: 0.0099\n",
      "Epoch [6/10], Loss: 0.0106\n",
      "Epoch [6/10], Loss: 0.0100\n",
      "Epoch [6/10], Loss: 0.0100\n",
      "Epoch [6/10], Loss: 0.0095\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0099\n",
      "Epoch [6/10], Loss: 0.0106\n",
      "Epoch [6/10], Loss: 0.0095\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0108\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0108\n",
      "Epoch [6/10], Loss: 0.0095\n",
      "Epoch [6/10], Loss: 0.0096\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0093\n",
      "Epoch [6/10], Loss: 0.0097\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0095\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0097\n",
      "Epoch [6/10], Loss: 0.0095\n",
      "Epoch [6/10], Loss: 0.0094\n",
      "Epoch [6/10], Loss: 0.0094\n",
      "Epoch [6/10], Loss: 0.0090\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0088\n",
      "Epoch [6/10], Loss: 0.0094\n",
      "Epoch [6/10], Loss: 0.0090\n",
      "Epoch [6/10], Loss: 0.0089\n",
      "Epoch [6/10], Loss: 0.0105\n",
      "Epoch [6/10], Loss: 0.0094\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0088\n",
      "Epoch [6/10], Loss: 0.0088\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0086\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0100\n",
      "Epoch [6/10], Loss: 0.0088\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0090\n",
      "Epoch [6/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0086\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0092\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0089\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0094\n",
      "Epoch [6/10], Loss: 0.0086\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0082\n",
      "Epoch [6/10], Loss: 0.0081\n",
      "Epoch [6/10], Loss: 0.0084\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0080\n",
      "Epoch [6/10], Loss: 0.0084\n",
      "Epoch [6/10], Loss: 0.0083\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0082\n",
      "Epoch [6/10], Loss: 0.0085\n",
      "Epoch [6/10], Loss: 0.0088\n",
      "Epoch [6/10], Loss: 0.0103\n",
      "Epoch [6/10], Loss: 0.0083\n",
      "Epoch [6/10], Loss: 0.0082\n",
      "Epoch [6/10], Loss: 0.0083\n",
      "Epoch [6/10], Loss: 0.0079\n",
      "Epoch [6/10], Loss: 0.0078\n",
      "Epoch [6/10], Loss: 0.0083\n",
      "Epoch [6/10], Loss: 0.0079\n",
      "Epoch [6/10], Loss: 0.0080\n",
      "Epoch [6/10], Loss: 0.0077\n",
      "Epoch [6/10], Loss: 0.0087\n",
      "Epoch [6/10], Loss: 0.0081\n",
      "Epoch [6/10], Loss: 0.0079\n",
      "Epoch [6/10], Loss: 0.0080\n",
      "Epoch [6/10], Loss: 0.0089\n",
      "Epoch [6/10], Loss: 0.0081\n",
      "Epoch [6/10], Loss: 0.0076\n",
      "Epoch [6/10], Loss: 0.0077\n",
      "Epoch [6/10], Loss: 0.0082\n",
      "Epoch [6/10], Loss: 0.0091\n",
      "Epoch [6/10], Loss: 0.0078\n",
      "Epoch [6/10], Loss: 0.0078\n",
      "Epoch [6/10], Loss: 0.0080\n",
      "Epoch [6/10], Loss: 0.0158\n",
      "Epoch [7/10], Loss: 0.0076\n",
      "Epoch [7/10], Loss: 0.0085\n",
      "Epoch [7/10], Loss: 0.0084\n",
      "Epoch [7/10], Loss: 0.0080\n",
      "Epoch [7/10], Loss: 0.0077\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0074\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0077\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0077\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0076\n",
      "Epoch [7/10], Loss: 0.0078\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0074\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0076\n",
      "Epoch [7/10], Loss: 0.0074\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0080\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0074\n",
      "Epoch [7/10], Loss: 0.0076\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0076\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0069\n",
      "Epoch [7/10], Loss: 0.0069\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0071\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0067\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0067\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0069\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0064\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0071\n",
      "Epoch [7/10], Loss: 0.0067\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0067\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0069\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0067\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0064\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0075\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0064\n",
      "Epoch [7/10], Loss: 0.0070\n",
      "Epoch [7/10], Loss: 0.0062\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0067\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0064\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0064\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0060\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0065\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0068\n",
      "Epoch [7/10], Loss: 0.0062\n",
      "Epoch [7/10], Loss: 0.0064\n",
      "Epoch [7/10], Loss: 0.0066\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0061\n",
      "Epoch [7/10], Loss: 0.0063\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [7/10], Loss: 0.0060\n",
      "Epoch [7/10], Loss: 0.0116\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0062\n",
      "Epoch [8/10], Loss: 0.0062\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0062\n",
      "Epoch [8/10], Loss: 0.0061\n",
      "Epoch [8/10], Loss: 0.0063\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0065\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0060\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0134\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0061\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0061\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0058\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0057\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0132\n",
      "Epoch [8/10], Loss: 0.0061\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0063\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [8/10], Loss: 0.0079\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0069\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0110\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0053\n",
      "Epoch [8/10], Loss: 0.0048\n",
      "Epoch [8/10], Loss: 0.0049\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0054\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0049\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0050\n",
      "Epoch [8/10], Loss: 0.0051\n",
      "Epoch [8/10], Loss: 0.0048\n",
      "Epoch [8/10], Loss: 0.0045\n",
      "Epoch [8/10], Loss: 0.0105\n",
      "Epoch [9/10], Loss: 0.0049\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0050\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0049\n",
      "Epoch [9/10], Loss: 0.0050\n",
      "Epoch [9/10], Loss: 0.0049\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0055\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0048\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0099\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0063\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0049\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0047\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0044\n",
      "Epoch [9/10], Loss: 0.0046\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0040\n",
      "Epoch [9/10], Loss: 0.0039\n",
      "Epoch [9/10], Loss: 0.0039\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0041\n",
      "Epoch [9/10], Loss: 0.0104\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0042\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0041\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0048\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0041\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0083\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0041\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0033\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0038\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0037\n",
      "Epoch [10/10], Loss: 0.0033\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0033\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0035\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0034\n",
      "Epoch [10/10], Loss: 0.0033\n",
      "Epoch [10/10], Loss: 0.0084\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_folder, img_ext, mask_folder, mask_ext, transform=None, mask_transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.img_ext = img_ext\n",
    "        self.mask_folder = mask_folder\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        self.filenames = [os.path.splitext(filename)[0] for filename in os.listdir(img_folder)\n",
    "                          if os.path.exists(os.path.join(mask_folder, os.path.splitext(filename)[0] + mask_ext))]\n",
    "\n",
    "    # ... rest of the class remains the same ...\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_folder, self.filenames[idx] + self.img_ext)\n",
    "        mask_path = os.path.join(self.mask_folder, self.filenames[idx] + self.mask_ext)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # assuming masks are in 'L' mode\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = torch.squeeze(mask, 0)  # remove the first dimension (1, H, W) -> (H, W)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Modified image size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Modified mask size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ... rest of the setup remains the same ...\n",
    "path_folder = '../9517proj_sources/train'\n",
    "\n",
    "path_folder_2 = '../9517proj_sources/off'\n",
    "\n",
    "train_dataset = CustomDataset(path_folder, '.jpg', path_folder_2, '.png', transform=transform, mask_transform=mask_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# create model\n",
    "model = deeplabv3_resnet50(pretrained=False, progress=True, num_classes=2, aux_loss=None)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(10):  # suppose we train for 10 epochs\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)['out']\n",
    "\n",
    "        # resize masks and outputs for the loss function\n",
    "        outputs = F.interpolate(outputs, size=(masks.shape[1], masks.shape[2]), mode='bilinear', align_corners=False)\n",
    "\n",
    "        loss = criterion(outputs, masks.long())  # note we need to convert masks to 'long' type\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch [{}/10], Loss: {:.4f}'.format(epoch + 1, loss.item()))\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.state_dict(), 'model_epoch_{}.pth'.format(epoch+1))\n",
    "\n",
    "# Load the model\n",
    "# Here we use the model from the last epoch, but you can replace '10' with any epoch number to load that model.\n",
    "model.load_state_dict(torch.load('model_epoch_10.pth'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T03:40:12.297183700Z",
     "start_time": "2023-07-24T03:29:33.830106400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T03:40:24.564732100Z",
     "start_time": "2023-07-24T03:40:24.374150700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "model.load_state_dict(torch.load('model_epoch_9.pth'))\n",
    "model.eval()\n",
    "# 如果你有可用的GPU，可以将模型放到GPU上\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "test_folder: LiteralString = '../9517proj_sources/valid/valid'\n",
    "test_ext = '.jpg'\n",
    "os.makedirs('output_folder', exist_ok=True)\n",
    "output_folder = '../9517proj_sources/output_folder'\n",
    "\n",
    "test_filenames = [os.path.splitext(filename)[0] for filename in os.listdir(test_folder)]\n",
    "\n",
    "with torch.no_grad():  # we don't need gradients for testing\n",
    "    for filename in test_filenames:\n",
    "        img_path = os.path.join(test_folder, filename + test_ext)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_size = (image.width, image.height)\n",
    "        image = transform(image)  # apply the same transform as during training\n",
    "        image = image.unsqueeze(0)  # add a batch dimension\n",
    "\n",
    "        image = image.to('cuda') if torch.cuda.is_available() else image\n",
    "\n",
    "        output = model(image)['out']\n",
    "        output = torch.argmax(output, dim=1)  # get the most likely prediction\n",
    "\n",
    "        # resize the output to match the original image size\n",
    "        output = cv2.resize(output[0].cpu().numpy(), orig_size, interpolation=cv2.INTER_NEAREST)\n",
    "        # print(np.unique(output))\n",
    "        # print(output)\n",
    "        # get the original image\n",
    "        orig_img = cv2.imread(img_path)\n",
    "\n",
    "        # apply the mask to the original image\n",
    "        segmented_img = np.zeros_like(orig_img)\n",
    "        for i in range(3):  # for each color channel\n",
    "            segmented_img[:, :, i] = np.where(output == 1, orig_img[:, :, i], 0)\n",
    "        # print(np.unique(segmented_img))\n",
    "        # save the segmented image\n",
    "        # segmented_img = (segmented_img * 255).astype(np.uint8)\n",
    "        res = cv2.imwrite(filename + '_segmented.jpg', segmented_img)\n",
    "        print(res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T03:42:43.544926300Z",
     "start_time": "2023-07-24T03:42:29.630968300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n",
      "Shape after model: torch.Size([1, 2, 32, 32])\n",
      "Shape after softmax: torch.Size([1, 2, 32, 32])\n",
      "Shape after argmax: torch.Size([1, 32, 32])\n",
      "Shape after squeeze and to numpy: (32, 32)\n",
      "Shape after resize: (640, 640)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load('model_epoch_10.pth'))\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "# Change the path as needed\n",
    "test_folder = '../9517proj_sources/valid/valid'\n",
    "test_ext = '.jpg'\n",
    "os.makedirs('output_folder', exist_ok=True)\n",
    "output_folder = '../9517proj_sources/output_folder'\n",
    "\n",
    "test_filenames = [os.path.splitext(filename)[0] for filename in os.listdir(test_folder)]\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)  # Softmax for converting output to probabilities\n",
    "\n",
    "with torch.no_grad():\n",
    "    for filename in test_filenames:\n",
    "        img_path = os.path.join(test_folder, filename + test_ext)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_size = (image.width, image.height)\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.to('cuda')\n",
    "\n",
    "        output = model(image)['out']\n",
    "        print(\"Shape after model:\", output.shape)\n",
    "\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        print(\"Shape after softmax:\", probs.shape)\n",
    "\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        print(\"Shape after argmax:\", output.shape)\n",
    "\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "        print(\"Shape after squeeze and to numpy:\", output.shape)\n",
    "\n",
    "        output = cv2.resize(output, orig_size, interpolation=cv2.INTER_NEAREST)\n",
    "        print(\"Shape after resize:\", output.shape)\n",
    "        output = np.squeeze(output)\n",
    "\n",
    "        for i in range(3):  # for each color channel\n",
    "            orig_img[:, :, i] = orig_img[:, :, i] * output\n",
    "            # Save the image\n",
    "        res = cv2.imwrite(filename + '_segmented.jpg', orig_img)\n",
    "        print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T04:16:16.940071900Z",
     "start_time": "2023-07-24T04:16:04.009089400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
